{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e8ae94-5199-4b41-808a-ab7031c723eb",
   "metadata": {},
   "source": [
    "# EDA notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfe915-46cc-4d83-a5f0-f81ee350a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17041328-46eb-4a26-a41a-57c6e4f4cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "sys.path.insert(0,\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ba12a-3f62-4c32-b840-535fa3504e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acf4b6-fe42-42e1-9266-0cf5ff28eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede46c7e-025f-48cb-bc22-880fd362f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options.train_options import TrainOptions\n",
    "opt = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3df3c-88eb-4807-a072-41c9b069d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to run on notebook\n",
    "opt.dataroot = \"../../datasets/acgpn_data/try_on_training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da1adf-5441-42f8-9ab1-97540cd20333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d7c96-93bd-4cac-9942-fb534ddc07d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7e72c-d7e1-478b-9eaf-36e64b24c236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c1259-c96f-45c1-a439-d900a9097127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5f80e-b1fc-437a-afea-26dcfb57f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n",
    "### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import ipdb\n",
    "\n",
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(BaseDataset, self).__init__()\n",
    "\n",
    "    def name(self):\n",
    "        return 'BaseDataset'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        pass\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt.resize_or_crop == 'resize_and_crop':\n",
    "        new_h = new_w = opt.loadSize            \n",
    "    elif opt.resize_or_crop == 'scale_width_and_crop':\n",
    "        new_w = opt.loadSize\n",
    "        new_h = opt.loadSize * h // w\n",
    "\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt.fineSize))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt.fineSize))\n",
    "    \n",
    "    #flip = random.random() > 0.5\n",
    "    flip = 0\n",
    "    return {'crop_pos': (x, y), 'flip': flip}\n",
    "\n",
    "def get_transform(opt, params, method=Image.BICUBIC, normalize=True):\n",
    "    transform_list = []\n",
    "    if 'resize' in opt.resize_or_crop:\n",
    "        osize = [opt.loadSize, opt.loadSize]\n",
    "        transform_list.append(transforms.Scale(osize, method))   \n",
    "    elif 'scale_width' in opt.resize_or_crop:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.loadSize, method)))\n",
    "        osize = [256,192]\n",
    "        transform_list.append(transforms.Scale(osize, method))  \n",
    "    if 'crop' in opt.resize_or_crop:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.fineSize)))\n",
    "\n",
    "    if opt.resize_or_crop == 'none':\n",
    "        base = float(2 ** opt.n_downsample_global)\n",
    "        if opt.netG == 'local':\n",
    "            base *= (2 ** opt.n_local_enhancers)\n",
    "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n",
    "\n",
    "    if opt.isTrain and not opt.no_flip:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def normalize():    \n",
    "    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "def __make_power_2(img, base, method=Image.BICUBIC):\n",
    "    ow, oh = img.size        \n",
    "    h = int(round(oh / base) * base)\n",
    "    w = int(round(ow / base) * base)\n",
    "    if (h == oh) and (w == ow):\n",
    "        return img\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    if (ow == target_width):\n",
    "        return img    \n",
    "    w = target_width\n",
    "    h = int(target_width * oh / ow)    \n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __crop(img, pos, size):\n",
    "    ow, oh = img.size\n",
    "    x1, y1 = pos\n",
    "    tw = th = size\n",
    "    if (ow > tw or oh > th):        \n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "    return img\n",
    "\n",
    "def __flip(img, flip):\n",
    "    if flip:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5084a-55be-458b-92b2-9f23ba040116",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n",
    "### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "import os.path\n",
    "#from data.base_dataset import BaseDataset, get_params, get_transform, normalize\n",
    "from data.image_folder import make_dataset, make_dataset_test\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from PIL import ImageDraw\n",
    "import ipdb\n",
    "\n",
    "class AlignedDataset(BaseDataset):\n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.root = opt.dataroot    \n",
    "        self.diction={}\n",
    "        ### input A (label maps)\n",
    "        if opt.isTrain or opt.use_encoded_image:\n",
    "            dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n",
    "            self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n",
    "            # All paths in train/test_label\n",
    "            self.A_paths = sorted(make_dataset(self.dir_A))\n",
    "            # All paths in train/test_label but random\n",
    "            self.AR_paths = make_dataset(self.dir_A)\n",
    "\n",
    "        self.fine_height=256\n",
    "        self.fine_width=192\n",
    "        self.radius=5\n",
    "        ### input A test (label maps)\n",
    "        if not (opt.isTrain or opt.use_encoded_image):\n",
    "            dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n",
    "            self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n",
    "            self.A_paths = sorted(make_dataset_test(self.dir_A))\n",
    "            dir_AR = '_AR' if self.opt.label_nc == 0 else '_labelref'\n",
    "            self.dir_AR = os.path.join(opt.dataroot, opt.phase + dir_AR)\n",
    "            self.AR_paths = sorted(make_dataset_test(self.dir_AR))\n",
    "\n",
    "        ### input B (real images)\n",
    "        dir_B = '_B' if self.opt.label_nc == 0 else '_img'\n",
    "        self.dir_B = os.path.join(opt.dataroot, opt.phase + dir_B)  \n",
    "        self.B_paths = sorted(make_dataset(self.dir_B))\n",
    "        self.BR_paths = sorted(make_dataset(self.dir_B))\n",
    "        \n",
    "        self.dataset_size = len(self.A_paths)\n",
    "        self.build_index(self.B_paths)\n",
    "\n",
    "        ### input E (edge_maps)\n",
    "        if opt.isTrain or opt.use_encoded_image:\n",
    "            dir_E = '_edge'\n",
    "            self.dir_E = os.path.join(opt.dataroot, opt.phase + dir_E)\n",
    "            self.E_paths = sorted(make_dataset(self.dir_E))\n",
    "            self.ER_paths = make_dataset(self.dir_E)\n",
    "\n",
    "        ### input M (masks)\n",
    "        if opt.isTrain or opt.use_encoded_image:\n",
    "            dir_M = '_mask'\n",
    "            self.dir_M = os.path.join(opt.dataroot, opt.phase + dir_M)\n",
    "            self.M_paths = sorted(make_dataset(self.dir_M))\n",
    "            self.MR_paths = make_dataset(self.dir_M)\n",
    "\n",
    "        ### input MC(color_masks)\n",
    "        if opt.isTrain or opt.use_encoded_image:\n",
    "            dir_MC = '_colormask'\n",
    "            self.dir_MC = os.path.join(opt.dataroot, opt.phase + dir_MC)\n",
    "            self.MC_paths = sorted(make_dataset(self.dir_MC))\n",
    "            self.MCR_paths = make_dataset(self.dir_MC)\n",
    "        \n",
    "        ### input C(color)\n",
    "        if opt.isTrain or opt.use_encoded_image:\n",
    "            dir_C = '_color'\n",
    "            self.dir_C = os.path.join(opt.dataroot, opt.phase + dir_C)\n",
    "            self.C_paths = sorted(make_dataset(self.dir_C))\n",
    "            self.CR_paths = make_dataset(self.dir_C)\n",
    "        # self.build_index(self.C_paths)\n",
    "        \n",
    "        \n",
    "        ### input A test (label maps)\n",
    "        if not (opt.isTrain or opt.use_encoded_image):\n",
    "            dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n",
    "            self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n",
    "            self.A_paths = sorted(make_dataset_test(self.dir_A))\n",
    "            \n",
    "    \n",
    "    def random_sample(self,item):\n",
    "        name = item.split('/')[-1]\n",
    "        name = name.split('-')[0]\n",
    "        lst=self.diction[name]\n",
    "        new_lst=[]\n",
    "        for dir in lst:\n",
    "            if dir != item:\n",
    "                new_lst.append(dir)\n",
    "        return new_lst[np.random.randint(len(new_lst))]\n",
    "    \n",
    "    def build_index(self,dirs):\n",
    "        for k,dir in enumerate(dirs):\n",
    "            name=dir.split('/')[-1]\n",
    "            name=name.split('-')[0]\n",
    "\n",
    "            # print(name)\n",
    "            for k,d in enumerate(dirs[max(k-20,0):k+20]):\n",
    "                if name in d:\n",
    "                    if name not in self.diction.keys():\n",
    "                        self.diction[name]=[]\n",
    "                        self.diction[name].append(d)\n",
    "                    else:\n",
    "                        self.diction[name].append(d)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        train_mask=9600\n",
    "        ipdb.set_trace()\n",
    "        breakpoint()\n",
    "        \n",
    "        ### input A (label maps)\n",
    "        # box=[]\n",
    "        # for k,x in enumerate(self.A_paths):\n",
    "        #     if '2372656' in x :\n",
    "        #         box.append(k)\n",
    "        # index=box[np.random.randint(len(box))]\n",
    "        print(index)\n",
    "        test=index#np.random.randint(10000)\n",
    "        A_path = self.A_paths[index]\n",
    "        AR_path = self.AR_paths[index]\n",
    "        A = Image.open(A_path).convert('L')\n",
    "        AR = Image.open(AR_path).convert('L')\n",
    "\n",
    "        params = get_params(self.opt, A.size)\n",
    "        if self.opt.label_nc == 0:\n",
    "            transform_A = get_transform(self.opt, params)\n",
    "            A_tensor = transform_A(A.convert('RGB'))\n",
    "            AR_tensor = transform_A(AR.convert('RGB'))\n",
    "        else:\n",
    "            transform_A = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n",
    "            A_tensor = transform_A(A) * 255.0\n",
    "            AR_tensor = transform_A(AR) * 255.0\n",
    "        B_tensor = inst_tensor = feat_tensor = 0\n",
    "        ### input B (real images)\n",
    "        B_path = self.B_paths[index]\n",
    "        BR_path = self.BR_paths[index]\n",
    "        B = Image.open(B_path).convert('RGB')\n",
    "        BR = Image.open(BR_path).convert('RGB')\n",
    "        transform_B = get_transform(self.opt, params)      \n",
    "        B_tensor = transform_B(B)\n",
    "        BR_tensor = transform_B(BR)\n",
    "\n",
    "        ### input M (masks)\n",
    "        M_path = self.M_paths[np.random.randint(12000)]\n",
    "        MR_path =self.MR_paths[np.random.randint(12000)]\n",
    "        M = Image.open(M_path).convert('L')\n",
    "        MR = Image.open(MR_path).convert('L')\n",
    "        M_tensor = transform_A(MR)\n",
    "\n",
    "        ### input_MC (colorMasks)\n",
    "        MC_path = B_path#self.MC_paths[1]\n",
    "        MCR_path = B_path#self.MCR_paths[1]\n",
    "        MCR = Image.open(MCR_path).convert('L')\n",
    "        MC_tensor = transform_A(MCR)\n",
    "\n",
    "        ### input_C (color)\n",
    "        # print(self.C_paths)\n",
    "        C_path = self.C_paths[test]\n",
    "        C = Image.open(C_path).convert('RGB')\n",
    "        C_tensor = transform_B(C)\n",
    "\n",
    "        ##Edge\n",
    "        E_path = self.E_paths[test]\n",
    "        # print(E_path)\n",
    "        E = Image.open(E_path).convert('L')\n",
    "        E_tensor = transform_A(E)\n",
    "\n",
    "\n",
    "        ##Pose\n",
    "        pose_name =B_path.replace('.png', '_keypoints.json').replace('.jpg','_keypoints.json').replace('train_img','train_pose')\n",
    "        with open(osp.join(pose_name), 'r') as f:\n",
    "            pose_label = json.load(f)\n",
    "            try:\n",
    "                pose_data = pose_label['people'][0]['pose_keypoints']\n",
    "            except IndexError:\n",
    "                pose_data = [0 for i in range(54)]\n",
    "            pose_data = np.array(pose_data)\n",
    "            pose_data = pose_data.reshape((-1,3))\n",
    "\n",
    "        point_num = pose_data.shape[0]\n",
    "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
    "        r = self.radius\n",
    "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
    "        pose_draw = ImageDraw.Draw(im_pose)\n",
    "        for i in range(point_num):\n",
    "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
    "            draw = ImageDraw.Draw(one_map)\n",
    "            pointx = pose_data[i,0]\n",
    "            pointy = pose_data[i,1]\n",
    "            if pointx > 1 and pointy > 1:\n",
    "                draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "                pose_draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "            one_map = transform_B(one_map.convert('RGB'))\n",
    "            pose_map[i] = one_map[0]\n",
    "        P_tensor=pose_map\n",
    "        if self.opt.isTrain:\n",
    "            input_dict = { 'label': A_tensor, 'label_ref': AR_tensor, 'image': B_tensor, 'image_ref': BR_tensor, 'path': A_path, 'path_ref': AR_path,\n",
    "                            'edge': E_tensor,'color': C_tensor, 'mask': M_tensor, 'colormask': MC_tensor,'pose':P_tensor\n",
    "                          }\n",
    "        else:\n",
    "            input_dict = {'label': A_tensor, 'label_ref': AR_tensor, 'image': B_tensor, 'image_ref': BR_tensor, 'path': A_path, 'path_ref': AR_path}\n",
    "\n",
    "        return input_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.A_paths) // self.opt.batchSize * self.opt.batchSize\n",
    "\n",
    "    def name(self):\n",
    "        return 'AlignedDataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace34b7-948f-4617-b186-35a1f15ed58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33557a80-ddff-4740-af0a-ff764eaf1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "#from data.base_data_loader import BaseDataLoader\n",
    "\n",
    "class BaseDataLoader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        pass\n",
    "\n",
    "    def load_data():\n",
    "        return None\n",
    "\n",
    "\n",
    "def CreateDataset(opt):\n",
    "    dataset = None\n",
    "    dataset = AlignedDataset()\n",
    "\n",
    "    print(\"dataset [%s] was created\" % (dataset.name()))\n",
    "    dataset.initialize(opt)\n",
    "    return dataset\n",
    "\n",
    "class CustomDatasetDataLoader(BaseDataLoader):\n",
    "    def name(self):\n",
    "        return 'CustomDatasetDataLoader'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        BaseDataLoader.initialize(self, opt)\n",
    "        self.dataset = CreateDataset(opt)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=opt.batchSize,\n",
    "            shuffle=not opt.serial_batches,\n",
    "            num_workers=0) # int(opt.nThreads) # changed for debugger\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.dataloader\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.opt.max_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aab4eb-fbe9-4dfd-bff6-192a07533064",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.nThreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2360ba-fec4-4e63-bd7c-90b73d045ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoader(opt):\n",
    "    data_loader = CustomDatasetDataLoader()\n",
    "    print(data_loader.name())\n",
    "    data_loader.initialize(opt)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a72d4-0d7c-4520-8b00-3970f451983b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf86696-f7b5-4243-b8d3-a2fd36e3e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "print('#training images = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752ed15-3c84-4955-b3d9-e357c712b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae7808-14c8-4806-8f73-9573c023d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0a2d3-fdc4-4e5f-b637-900a9b49ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe0bbc-3a00-4b72-886b-b29ad9910d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec009a04-72f2-470a-b527-8609ad730009",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    if i == 1:\n",
    "        print(\"Exiting loop.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb139-2f3b-4b68-903d-0ad507a3e708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91623efc-d8cf-40c2-932b-4b1e16c6248d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193addc-8b01-41ea-abc4-18c9e8e85e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfa304-1cc0-4f67-ab25-8264454f7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87305dbf-3852-4358-95d7-7b9f6f8e1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = cv2.imread('../../datasets/acgpn_data/try_on_training/train_label/014857_0.png', -1)\n",
    "AR = cv2.imread('../../datasets/acgpn_data/try_on_training/train_label/003922_0.png', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f571cb-f799-4182-983d-e57df3ee2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208c031-acc0-40ed-985f-9a6e0fc9274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed53a2c-478b-42c9-bb17-af107023cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = cv2.cvtColor(cv2.cv2.imread('../../datasets/acgpn_data/try_on_training/train_img/014857_0.jpg'), cv2.COLOR_BGR2RGB)\n",
    "BR = cv2.cvtColor(cv2.cv2.imread('../../datasets/acgpn_data/try_on_training/train_img/014857_0.jpg'), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcc5e6-b387-49b5-a13e-41d0c16489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b717a3-b38c-44c0-82d5-722252128481",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = cv2.imread('../../datasets/acgpn_data/try_on_training/train_mask/09851.png', -1)\n",
    "MR = cv2.imread('../../datasets/acgpn_data/try_on_training/train_mask/01482.png', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85755ef3-1550-41d8-89ca-ab092d98c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d25046-157b-4643-b72d-002913c4250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a35de-6c1a-4d14-843b-108474757e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCR = cv2.cvtColor(cv2.cv2.imread('../../datasets/acgpn_data/try_on_training/train_img/014857_0.jpg'), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a1d6e-6abe-41f0-87de-a25457483a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da40fb-7139-41be-a3df-1193ce8ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = cv2.cvtColor(cv2.cv2.imread('../../datasets/acgpn_data/try_on_training/train_color/014857_1.jpg'), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe84c47-f90b-4199-bec2-5f687e66f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ccfa3-7178-481e-995c-2bdfc792d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = cv2.cvtColor(cv2.cv2.imread('../../datasets/acgpn_data/try_on_training/train_edge/014857_1.jpg'), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffb7c1-f060-45b1-93a4-0301a200869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_name = '../../datasets/acgpn_data/try_on_training/train_pose/014857_0_keypoints.json'\n",
    "pose_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc862c-aec2-4483-89bb-6a24e08babab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(osp.join(pose_name), 'r') as f:\n",
    "    pose_label = json.load(f)\n",
    "    try:\n",
    "        pose_data = pose_label['people'][0]['pose_keypoints']\n",
    "    except IndexError:\n",
    "        pose_data = [0 for i in range(54)]\n",
    "    pose_data = np.array(pose_data)\n",
    "    pose_data = pose_data.reshape((-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84f633-867e-4aef-a2a2-e47e706046c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347c75f-ad17-4ee9-8b3f-c2f9f810ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a86634-6cd2-43b0-8082-443bc09dcf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_num = pose_data.shape[0]\n",
    "point_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd1cee-bc79-4782-b69c-c5cb41cdfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pose_map = torch.zeros(point_num, 256, 192)\n",
    "pose_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d006b1-d7ae-471a-8c40-063b826c9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radius\n",
    "r = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156dc9b1-e6f3-41b5-a0b0-471bbce530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pose = Image.new('L', (256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec435402-1ded-4d88-8167-3d3476493a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_draw = ImageDraw.Draw(im_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1db4ed-dc54-4be0-bc24-36130749465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import ipdb\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt.resize_or_crop == 'resize_and_crop':\n",
    "        new_h = new_w = opt.loadSize            \n",
    "    elif opt.resize_or_crop == 'scale_width_and_crop':\n",
    "        new_w = opt.loadSize\n",
    "        new_h = opt.loadSize * h // w\n",
    "\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt.fineSize))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt.fineSize))\n",
    "    \n",
    "    #flip = random.random() > 0.5\n",
    "    flip = 0\n",
    "    return {'crop_pos': (x, y), 'flip': flip}\n",
    "\n",
    "def get_transform(opt, params, method=Image.BICUBIC, normalize=True):\n",
    "    transform_list = []\n",
    "    if 'resize' in opt.resize_or_crop:\n",
    "        osize = [opt.loadSize, opt.loadSize]\n",
    "        transform_list.append(transforms.Scale(osize, method))   \n",
    "    \n",
    "    elif 'scale_width' in opt.resize_or_crop:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.loadSize, method)))\n",
    "        osize = [256,192]\n",
    "        transform_list.append(transforms.Scale(osize, method))  \n",
    "    \n",
    "    if 'crop' in opt.resize_or_crop:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.fineSize)))\n",
    "\n",
    "    if opt.resize_or_crop == 'none':\n",
    "        base = float(2 ** opt.n_downsample_global)\n",
    "        if opt.netG == 'local':\n",
    "            base *= (2 ** opt.n_local_enhancers)\n",
    "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n",
    "\n",
    "    if opt.isTrain and not opt.no_flip:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    if (ow == target_width):\n",
    "        return img    \n",
    "    w = target_width\n",
    "    h = int(target_width * oh / ow)    \n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __flip(img, flip):\n",
    "    if flip:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n",
    "\n",
    "params = get_params(opt,(192, 256))\n",
    "transform_B = get_transform(opt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802e881-7ec5-4b03-9893-226f35b6a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.resize_or_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e716e47-8555-4ebd-beac-164dfdfded52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(point_num):\n",
    "    one_map = Image.new('L', (256, 192))\n",
    "    draw = ImageDraw.Draw(one_map)\n",
    "    pointx = pose_data[i,0]\n",
    "    pointy = pose_data[i,1]\n",
    "    if pointx > 1 and pointy > 1:\n",
    "        draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "        pose_draw.rectangle((pointx-r, pointy-r, pointx+r, pointy+r), 'white', 'white')\n",
    "    \n",
    "    one_map = transform_B(one_map.convert('RGB'))\n",
    "    pose_map[i] = one_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc71369-f9b6-4ff8-9791-55d5b85821be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd882d3-970e-4c15-bd6b-e26805e9a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(pose_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b36a31-a99d-4879-8803-5aee7f265c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, A: part segmentation of target person image\n",
    "# label_ref, AR: part segmentation of random person image\n",
    "# image, B: target person image\n",
    "# image_ref, BR: target person image\n",
    "# path, path of A\n",
    "# path_ref, path of AR\n",
    "# edge, E: binary segmentation mask of cloth\n",
    "# color, C: cloth image\n",
    "# mask, random image from train_mask folder\n",
    "# colormask, target person image\n",
    "# pose, the pose map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b61736-8a15-4cc6-adb6-2917f71f50f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f992f7db62b59d8ee09ff6871d0e5d93fda538d5e46ca9f3d81cc10adfb1560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
